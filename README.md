# ğŸ“˜ Automated Text Summarization â€“ Complete ML Pipeline

A **productionâ€‘ready endâ€‘toâ€‘end Machine Learning pipeline** for **Abstractive Text Summarization** built using **Hugging Face Transformers, Pegasus model, MLOps principles, and modular architecture**.

This project demonstrates how an industryâ€‘grade NLP system is designed â€” from **data ingestion to model evaluation** â€” following clean coding standards and scalable ML engineering practices.

---

## ğŸš€ Project Overview

Text summarization is a core NLP problem with applications in:

* News summarization
* Chat & conversation summarization
* Document understanding
* Knowledge extraction

In this project, we fineâ€‘tune the **Pegasus model** on the **SAMSum dataset** using a **fully modular, configurable, and reproducible ML pipeline**.

---

## ğŸ§  Key Features

* âœ… Endâ€‘toâ€‘end ML pipeline (Training + Evaluation)
* âœ… Modular folder structure (industry standard)
* âœ… Hugging Face Pegasus model
* âœ… SAMSum dialogue summarization dataset
* âœ… Custom exception & logging system
* âœ… Configâ€‘driven execution (YAML based)
* âœ… MLOpsâ€‘ready (easy MLflow / CIâ€‘CD integration)

---

## ğŸ—‚ï¸ Project Structure

```
Automated-Text-Summarization-Complete-ML-Pipeline
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ params.yaml
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ textSummarizer/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â””â”€â”€ model_evaluation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ configuration.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â””â”€â”€ entity_config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ commons.py
â”‚   â”‚
â”‚   â”œâ”€â”€ logger/
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚
â”‚   â””â”€â”€ exceptions/
â”‚       â””â”€â”€ exception.py
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ data_transformation/
â”‚   â”œâ”€â”€ model_trainer/
â”‚   â””â”€â”€ model_evaluation/
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

| Category      | Tools                           |
| ------------- | ------------------------------- |
| Language      | Python 3.9+                     |
| Deep Learning | PyTorch                         |
| NLP           | Hugging Face Transformers       |
| Model         | Google Pegasus                  |
| Dataset       | SAMSum                          |
| MLOps         | Modular Pipelines, YAML configs |
| Logging       | Custom Logging Module           |

---

## ğŸ“¦ Dataset

**SAMSum Dataset**

* Dialogueâ€‘based conversation summaries
* Used for abstractive summarization tasks

Source:

```
https://raw.githubusercontent.com/krishnaik06/datasets/main/summarizer-data.zip
```

---

## ğŸ”§ Configuration Files

### `config/config.yaml`

Controls:

* Dataset URL
* Artifact paths
* Model directories

### `params.yaml`

Controls:

* Model checkpoint
* Tokenizer name
* Training hyperparameters

---

## ğŸ” Pipeline Flow

```
main.py
   â†“
TrainingPipeline
   â†“
Data Ingestion
   â†“
Data Transformation (Tokenization)
   â†“
Model Training (Pegasus)
   â†“
Model Evaluation (ROUGE)
```

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Run Training Pipeline

```bash
python main.py
```

Artifacts will be created automatically inside the `artifacts/` folder.

---

## ğŸ“Š Model Evaluation

* Metric Used: **ROUGE**
* Scores saved as CSV
* Evaluated on test split

Example metrics:

```
rouge1
rouge2
rougeL
rougeLsum
```

---

## ğŸ§ª Error Handling & Logging

* Centralized logging system
* Custom exception class
* Full traceback support
* Productionâ€‘safe failure handling

---

## ğŸ§  ML Engineering Best Practices Used

* Clean architecture
* Configâ€‘driven pipelines
* Separation of concerns
* Reproducibility
* Artifact versioning
* GPUâ€‘aware training

---

## ğŸ“Œ Future Improvements

* ğŸ”¹ MLflow experiment tracking
* ğŸ”¹ FastAPI inference service
* ğŸ”¹ Docker & AWS deployment
* ğŸ”¹ CI/CD pipeline
* ğŸ”¹ UI using Streamlit

---

## ğŸ‘¨â€ğŸ’» Author

**Mohammad Shuaib**
Certified Data Scientist | ML Engineer | NLP Enthusiast

* Expertise: Data Science, Machine Learning, Deep Learning, MLOps, Generative AI
* Passionate about building scalable ML systems

---

## â­ If You Like This Project

Give it a â­ on GitHub â€” it motivates continuous improvement!

---

## ğŸ“œ License

This project is licensed for educational and research purposes.

---

> *"Good ML models are trained. Great ML systems are engineered."* ğŸš€
